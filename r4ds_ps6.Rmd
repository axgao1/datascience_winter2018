---
title: "r4ds_ps6"
author: "Annie Gao"
date: "2/25/2018"
output: html_document
---

```{r setup, include = FALSE}
library(tidyverse)
library(forcats)
library(knitr)
library(lubridate)
#install.packages("kableExtra")
library(kableExtra)
```

# Prelim questions
##1
I worked with Umer Naeem.

##2
I spent 10 hours on this part and 20 hours waiting for code to run on the Transit portion. 

#1 R4DS Chapter 15

##1
```{r, error = FALSE}
x <- c("Dec", "Apr", "Jan", "Mar")

factor(x, levels = unique(x))
factor(x, levels = x)

x_dups <- c("Dec", "Apr", "Jan", "Mar", "Mar", "Dec")

factor(x_dups, levels = unique(x_dups))
#factor(x_dups, levels = x_dups) commented because will return factor level duplicate error
```

The first output with unique produces the same output as the second without the unique operator. All the x values are unique already and therefore did not require the unique operator. In the vector x_dups, including the unique operator returns non-duplicate levels. Without the unique operator, R throws an error saying that levels are duplicates.  

##2
```{r}
forcats::gss_cat
summary(gss_cat)
levels(forcats::gss_cat$rincome)

ggplot(gss_cat, mapping = aes(rincome)) + 
  geom_bar()

#two improvements
ggplot(gss_cat, mapping = aes(fct_collapse(rincome, other = c("Not applicable", 
                                                                "No answer",
                                                                "Don't know",
                                                                "Refused")))) +
  geom_bar() + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

The GSS income measurements have historically been recorded like this where the lower income levels cover larger intervals. All the labels on the x axis overlap making it difficult to read. The bin size on the x axis also are not consistent, making it difficult to interpret frequency because the same income group sizes are different and the order of the levels are not in order. The two changes I made were reordering the levels and moving all the non-answers into "other" and angling the labels so that they don't overlap and are readable.

##3
```{r}
gss_cat %>% 
  count(relig) %>%
  arrange(desc(n))

gss_cat %>% 
  count(partyid) %>%
  arrange(desc(n))
```

The most common religion in this dataset is Protestant. The most common partid is Independent.

##4
```{r, warning = FALSE}
levels(gss_cat$relig)
levels(gss_cat$denom)

#method 1
dont_know <- c("No answer", "Other", "Don't know", 
               "Not applicable", "No denomination")

relig_denom_kable <- gss_cat %>%
  filter(!denom %in% dont_know) %>%
  count(relig, denom) %>%
  arrange(desc(n))

knitr::kable(relig_denom_kable, "markdown") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))

#method 2
relig_denom <- gss_cat %>%
  filter(!denom %in% dont_know) %>%
  count(relig, denom)

ggplot(relig_denom, aes(x = relig, y = denom)) +
  geom_point()
```

Protestant is the only religion that the factor denom applies to after taking out no answers and other noninformative denom responses. Both the table and the visualization show that only Protestant is associated with a denomination.

##5
```{r, warning = FALSE}
ggplot(gss_cat, aes(x = tvhours)) +
  geom_histogram(binwidth = 1)

#drop outliers
gss_cat_clean <- subset(gss_cat, tvhours <= 12 & !is.na(tvhours))
  
relig_summary <- gss_cat_clean %>%
  group_by(relig) %>%
  summarise(avg_tvhours = mean(tvhours, na.rm = TRUE),
            n = n())

relig_summary_reorder <- relig_summary %>%
  mutate(relig_reorder = fct_reorder(relig, avg_tvhours, median)) 

ggplot(relig_summary_reorder, aes(avg_tvhours, relig_reorder)) + 
  geom_point()
```

Any number above 12 can be considered unreasonably high. We should drop these rows because if we're trying to examine tv watching hours by religion, it would be misleading and erroneous to try to recode these outliers to another more reasonable hour because it forces the religion associated with the tv outlier to take on a different weight when examining tv hours. It causes the relationship between tv hours and religion to be imprecise. It seems that, on average, Protestants watch the highest number of hours of tv with Catholics at the second highest and other Eastern religions at the lowest number of hours of tv watching.

##6
```{r}
levels(gss_cat$marital)
ggplot(gss_cat, aes(marital)) +
  geom_bar()

levels(gss_cat$race)
ggplot(gss_cat, aes(race)) +
  geom_bar()

levels(gss_cat$rincome)
ggplot(gss_cat, aes(rincome)) +
  geom_bar()

levels(gss_cat$partyid)
ggplot(gss_cat, aes(partyid)) +
  geom_bar()

levels(gss_cat$relig)
ggplot(gss_cat, aes(relig)) +
  geom_bar()

levels(gss_cat$denom)
ggplot(gss_cat, aes(denom)) +
  geom_bar()
```

For the marital factor, the levels are principled. The subdivisions are standard. For the the race factor, the levels are principled as the categories are ordered by increasing number of observations. For the rincome factor, the levels are arbitrary. The sizes of the subdivisions differ for several levels and the subdivisions are not standard with spikes on both ends of the spectrum. For partyid, the levels are principled, although the existence of "Not str democrat" and "Not str republican" might make the levels more arbitray as there is no clear cut off. For the factor relig, the levels are arbitrary. The ordering doesnt reflect any particular arrangement of religions. For the factor denom, the levels seem somewhat arbitrary since there is no specific ordering again associated with different denominations.

##7
Moving “Not applicable” to the front with the other special levels essentially gives it an ordering of 1, so that when graphed, the y axis would start with 1 which corresponds with "Not applicable."

##8
```{r}
partyid_trend <- gss_cat %>%
  mutate(partyid_coll = fct_collapse(partyid, 
                                     dems = c("Strong democrat", "Not str democrat"),
                                     reps = c("Strong republican", "Not str republican"),
                                     indep = c("Independent", "Ind,near rep", "Ind,near dem"),
                                     other = c("No answer", "Don't know", "Other party")))

party_trend_plot <- partyid_trend %>% 
  group_by(year) %>%
  count(partyid_coll) %>%
  mutate(party_prop = n/sum(n))

ggplot(party_trend_plot, aes(x = year, y = party_prop, color = partyid_coll)) + 
  geom_point() + 
  geom_smooth()
```

The proportion of Republicans has steadily decreased over the years from approximately 2004 to 2014. The proportion of Independent identifying people has seen a slight increase during approximately the same period. The proprtion of Democrats mostly did not change notwithstanding a slight increase in 2010. 

##9
```{r}
levels(gss_cat$rincome)

gss_cat %>% 
  mutate(rincome_cat = fct_collapse(rincome,
               other = c("No answer", "Don't know", "Refused", "Not applicable"),
               `lt1000` = c("Lt $1000"),
               `1000-4999` = c("$1000 to 2999", "$3000 to 3999", "$4000 to 4999"),
               `5000-9999` = c("$5000 to 5999", "$6000 to 6999", "$7000 to 7999", "$8000 to 9999"),
               `10000-14999` = c("$10000 - 14999"),
               `15000-19999` = c("$15000 - 19999"),
               `20000-24999` = c("$20000 - 24999"),
               `25000higher` = c("$25000 or more")))
```

I collapsed rincome into intervals of 5000 starting with non informative reports, lower than 1000, going up to 24999 and 25000 and higher.

#2 R4DS Chapter 16

##1
```{r}
library(nycflights13)

#from book
make_datetime_100 <- function(year, month, day, time) {
  make_datetime(year, month, day, time %/% 100, time %% 100)
}

flights_dt <- flights %>% 
  filter(!is.na(dep_time), !is.na(arr_time)) %>% 
  mutate(
    dep_time = make_datetime_100(year, month, day, dep_time),
    arr_time = make_datetime_100(year, month, day, arr_time),
    sched_dep_time = make_datetime_100(year, month, day, sched_dep_time),
    sched_arr_time = make_datetime_100(year, month, day, sched_arr_time)
  ) %>% 
  select(origin, dest, ends_with("delay"), ends_with("time"))

#compare
flights_dt <- flights_dt %>% 
  mutate(dep_delay_conv = (dep_time - sched_dep_time)/60)

table(ifelse(flights_dt$dep_delay_conv == flights_dt$dep_delay, 
             "equal", "not equal"))
```

Using code from the book to convert departure and arrival times, I examined whether the newly converted delay times are the same as the difference in departure and scheduled departure times. After doing the conversion and converting to minutes to match the original departure delay, the two columns are mostly the same.

##2
```{r}
flights_dt <- flights_dt %>%
  mutate(air_time_conv = arr_time - dep_time, 
         air_time_conv2 = sched_arr_time - sched_dep_time)

table(ifelse(flights_dt$air_time_conv2 == flights_dt$air_time, 
             "equal", "not equal"))
```

It seems that most of the flights' air times do not equal the time difference between arrival time and departure time. I tried to find the air time using scheduled arrival and departure times and non-scheduled arrival and departure times. Neither yielded results that showed it was equivalent to the original air time.

##3
```{r, warning = FALSE, error = TRUE}
d1 <- "January 1, 2003"
d2 <- "2013-Mar-07"
d3 <- "06-Jun-2017"
d4 <- c("August 9 (2015)", "July 1 (2015)") 
d5 <- "12/29/14" # Dec 29, 2014

mdy(d1)
ymd(d2)
dmy(d3)
mdy(d4)
mdy(d5)
```

##4
```{r}
flight_halves <- flights_dt %>%
  mutate(hour = hour(dep_time),
         year_halves = ifelse(month(dep_time) <= 6, "first_half", "second_half")) 

ggplot(flight_halves, aes(hour, group = year_halves, color = year_halves)) +
  geom_freqpoly(binwidth = 1)
```

The distribution seems to trace out the flights throughout the day similarly between the first half and second half of the year. The distributions are roughly the same between the two halves of the year with the second half having slightly less variable than the first half around 12 noon on a day.

##5
```{r}
avg_delay_day <- flights_dt %>%
  mutate(sched_dep_hr = hour(sched_dep_time)) %>%
  group_by(sched_dep_hr) %>%
  summarise(avg_dep_delay = mean(dep_delay))

ggplot(avg_delay_day, aes(x = sched_dep_hr, y = avg_dep_delay)) +
  geom_point() +
  geom_smooth(se = FALSE)
```

The graph shows departure delays occur more as the day goes on in an exponential growth rate and peaks in the evening at around 7 or 8 pm and then drops after those evening hours. We should use sched_dep_time in order to measure against real departure delays because comparing scheduled departure to average delays would give a true representation of delays. Using actual departure time would underestimate the average delays since delays are already built into the departure time.

##6
```{r}
flights_dt %>%
  mutate(day_leave = wday(sched_dep_time)) %>%
  group_by(day_leave) %>%
  summarise(avg_dep_delay = mean(dep_delay, na.rm = TRUE)) %>%
  arrange(avg_dep_delay)
```

To minimise the chance of a delay, you should leave on the 7th day of the week, Saturday because it has the shortest average departure delay time.

##7
```{r}
ggplot(diamonds, mapping = aes(x= carat)) + 
  geom_histogram()

ggplot(flights, mapping = aes(x = sched_dep_time)) +
  geom_histogram()
```

In both diamonds and flights, the carats and sched_dep_time both have more values at rounded off numbers. Carats are concentrated at 0.5 1, 1.5, and 2 carats. Scheduled departure times seem to be concentrated at intervals of 15 minutes, or quarter hour, of each hour.

##8
```{r}
dates_vec_2015 <- ymd("2015-01-01") + months(0:11)
dates_vec_2018 <- ymd("2018-01-01") + months(0:11)
```

##9
```{r}
age_func <- function(birthday) {
  bday_converted <- ymd(birthday)
  my_age <- today() - bday_converted
  my_age <- as.duration(my_age)
  print(my_age)
}

age_func(19901223)
```

I wrote a function that converts my birthday in the ymd format to years by subtracting today's date and the reformatted birthday and then converting it from the difference in days to years using as.duration.

##10
```{r}
locale("zh")
parse_date("十二月 23 1990", "%B %d %Y", locale = locale("zh"))

locale("fi")
parse_date("joulukuuta 23 1990", "%B %d %Y", locale = locale("fi"))
```

In Chinese and Finnish, I printed a date object with month, calendar day, and year.